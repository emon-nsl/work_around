{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '+': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11}\n",
      "indices_char {0: ' ', 1: '+', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9'}\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        print(self.char_indices)\n",
    "        print('indices_char', self.indices_char)\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 772)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda: int(\n",
    "    \"\".join(\n",
    "        np.random.choice(list(\"0123456789\"))\n",
    "        for i in range(np.random.randint(1, DIGITS + 1))\n",
    "    )\n",
    ")\n",
    "a, b = f(), f()\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '+': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11}\n",
      "indices_char {0: ' ', 1: '+', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9'}\n",
      "Generating data...\n",
      "Total questions: 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "#         print(self.char_indices)\n",
    "#         print('indices_char', self.indices_char)\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, DIGITS + 1))\n",
    "        )\n",
    "    )\n",
    "    #f() is the function/lambda function that randomly takes a number between 1 to 4\n",
    "    #and generates a number of that length which is also random\n",
    "    a, b = f(), f() #generate two distinct integers\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}+{}\".format(a, b)\n",
    "    query = q + \" \" * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print(\"Total questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "# print(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Build model...\")\n",
    "num_layers = 1  # Try to add more LSTM layers!\n",
    "\n",
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1)) \n",
    "#RepeatVector repeats the last vector to given times\n",
    "#if previous shape is (none, 16) then if RepeatVector(4) is called\n",
    "#then the shape becomes (none, 4, 16)\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(num_layers):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n",
      "1407/1407 [==============================] - 28s 17ms/step - loss: 1.7471 - accuracy: 0.3594 - val_loss: 1.5369 - val_accuracy: 0.4168\n",
      "Q 3+19    T 22   ☒ 11  \n",
      "Q 989+216 T 1205 ☒ 1110\n",
      "Q 516+961 T 1477 ☒ 1441\n",
      "Q 977+667 T 1644 ☒ 1441\n",
      "Q 96+965  T 1061 ☑ 1061\n",
      "Q 831+40  T 871  ☒ 933 \n",
      "Q 95+929  T 1024 ☒ 1090\n",
      "Q 316+781 T 1097 ☒ 1110\n",
      "Q 419+359 T 778  ☒ 500 \n",
      "Q 78+92   T 170  ☒ 998 \n",
      "\n",
      "Iteration 2\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3476 - accuracy: 0.4976 - val_loss: 1.1668 - val_accuracy: 0.5710\n",
      "Q 392+5   T 397  ☒ 394 \n",
      "Q 939+89  T 1028 ☒ 1020\n",
      "Q 58+738  T 796  ☒ 809 \n",
      "Q 787+55  T 842  ☒ 823 \n",
      "Q 868+5   T 873  ☒ 880 \n",
      "Q 44+97   T 141  ☒ 156 \n",
      "Q 50+32   T 82   ☒ 60  \n",
      "Q 726+92  T 818  ☒ 810 \n",
      "Q 948+396 T 1344 ☒ 1390\n",
      "Q 64+139  T 203  ☑ 203 \n",
      "\n",
      "Iteration 3\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.0535 - accuracy: 0.6098 - val_loss: 0.9743 - val_accuracy: 0.6420\n",
      "Q 121+6   T 127  ☒ 129 \n",
      "Q 726+69  T 795  ☒ 797 \n",
      "Q 36+667  T 703  ☒ 795 \n",
      "Q 18+19   T 37   ☒ 33  \n",
      "Q 23+541  T 564  ☒ 569 \n",
      "Q 812+38  T 850  ☒ 841 \n",
      "Q 33+34   T 67   ☒ 73  \n",
      "Q 435+35  T 470  ☒ 479 \n",
      "Q 18+968  T 986  ☒ 995 \n",
      "Q 608+27  T 635  ☒ 631 \n",
      "\n",
      "Iteration 4\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.8957 - accuracy: 0.6700 - val_loss: 0.8477 - val_accuracy: 0.6852\n",
      "Q 41+885  T 926  ☒ 923 \n",
      "Q 579+46  T 625  ☒ 623 \n",
      "Q 15+347  T 362  ☒ 350 \n",
      "Q 58+66   T 124  ☒ 121 \n",
      "Q 958+82  T 1040 ☒ 1039\n",
      "Q 929+545 T 1474 ☒ 1478\n",
      "Q 21+665  T 686  ☒ 683 \n",
      "Q 477+41  T 518  ☒ 513 \n",
      "Q 22+567  T 589  ☒ 587 \n",
      "Q 93+682  T 775  ☒ 778 \n",
      "\n",
      "Iteration 5\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.7802 - accuracy: 0.7148 - val_loss: 0.7394 - val_accuracy: 0.7332\n",
      "Q 18+988  T 1006 ☒ 100 \n",
      "Q 67+333  T 400  ☒ 401 \n",
      "Q 124+75  T 199  ☒ 193 \n",
      "Q 533+525 T 1058 ☒ 1059\n",
      "Q 608+253 T 861  ☒ 869 \n",
      "Q 284+441 T 725  ☑ 725 \n",
      "Q 391+101 T 492  ☒ 490 \n",
      "Q 448+11  T 459  ☒ 451 \n",
      "Q 392+5   T 397  ☒ 399 \n",
      "Q 582+305 T 887  ☒ 891 \n",
      "\n",
      "Iteration 6\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.6858 - accuracy: 0.7504 - val_loss: 0.6552 - val_accuracy: 0.7576\n",
      "Q 169+576 T 745  ☒ 746 \n",
      "Q 577+50  T 627  ☒ 629 \n",
      "Q 66+89   T 155  ☒ 156 \n",
      "Q 16+978  T 994  ☒ 990 \n",
      "Q 49+9    T 58   ☒ 55  \n",
      "Q 56+54   T 110  ☒ 119 \n",
      "Q 21+91   T 112  ☑ 112 \n",
      "Q 312+82  T 394  ☒ 393 \n",
      "Q 924+626 T 1550 ☒ 1559\n",
      "Q 39+265  T 304  ☒ 306 \n",
      "\n",
      "Iteration 7\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.4870 - accuracy: 0.8201 - val_loss: 0.3535 - val_accuracy: 0.8802\n",
      "Q 801+35  T 836  ☑ 836 \n",
      "Q 443+59  T 502  ☑ 502 \n",
      "Q 93+358  T 451  ☒ 442 \n",
      "Q 515+530 T 1045 ☒ 1046\n",
      "Q 9+598   T 607  ☒ 606 \n",
      "Q 39+391  T 430  ☒ 429 \n",
      "Q 641+3   T 644  ☑ 644 \n",
      "Q 57+263  T 320  ☑ 320 \n",
      "Q 817+866 T 1683 ☑ 1683\n",
      "Q 49+847  T 896  ☑ 896 \n",
      "\n",
      "Iteration 8\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 0.2485 - accuracy: 0.9269 - val_loss: 0.1855 - val_accuracy: 0.9518\n",
      "Q 25+785  T 810  ☑ 810 \n",
      "Q 696+392 T 1088 ☒ 908 \n",
      "Q 738+9   T 747  ☑ 747 \n",
      "Q 856+0   T 856  ☑ 856 \n",
      "Q 104+105 T 209  ☒ 208 \n",
      "Q 398+41  T 439  ☑ 439 \n",
      "Q 97+696  T 793  ☒ 792 \n",
      "Q 984+87  T 1071 ☑ 1071\n",
      "Q 832+985 T 1817 ☒ 1818\n",
      "Q 708+496 T 1204 ☑ 1204\n",
      "\n",
      "Iteration 9\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.1410 - accuracy: 0.9648 - val_loss: 0.1885 - val_accuracy: 0.9375\n",
      "Q 284+43  T 327  ☑ 327 \n",
      "Q 161+10  T 171  ☑ 171 \n",
      "Q 55+860  T 915  ☑ 915 \n",
      "Q 508+3   T 511  ☑ 511 \n",
      "Q 650+950 T 1600 ☑ 1600\n",
      "Q 5+315   T 320  ☑ 320 \n",
      "Q 859+219 T 1078 ☑ 1078\n",
      "Q 51+872  T 923  ☑ 923 \n",
      "Q 868+877 T 1745 ☑ 1745\n",
      "Q 483+656 T 1139 ☑ 1139\n",
      "\n",
      "Iteration 10\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 0.0860 - accuracy: 0.9799 - val_loss: 0.0620 - val_accuracy: 0.9873\n",
      "Q 198+47  T 245  ☑ 245 \n",
      "Q 48+70   T 118  ☑ 118 \n",
      "Q 652+354 T 1006 ☑ 1006\n",
      "Q 23+87   T 110  ☑ 110 \n",
      "Q 430+618 T 1048 ☑ 1048\n",
      "Q 637+731 T 1368 ☑ 1368\n",
      "Q 20+923  T 943  ☑ 943 \n",
      "Q 48+737  T 785  ☑ 785 \n",
      "Q 70+432  T 502  ☑ 502 \n",
      "Q 111+730 T 841  ☑ 841 \n",
      "\n",
      "Iteration 11\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0692 - accuracy: 0.9822 - val_loss: 0.1619 - val_accuracy: 0.9415\n",
      "Q 70+749  T 819  ☑ 819 \n",
      "Q 386+262 T 648  ☒ 748 \n",
      "Q 81+681  T 762  ☑ 762 \n",
      "Q 67+909  T 976  ☑ 976 \n",
      "Q 868+133 T 1001 ☒ 991 \n",
      "Q 4+827   T 831  ☑ 831 \n",
      "Q 36+333  T 369  ☑ 369 \n",
      "Q 372+45  T 417  ☑ 417 \n",
      "Q 68+811  T 879  ☑ 879 \n",
      "Q 115+71  T 186  ☑ 186 \n",
      "\n",
      "Iteration 12\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 0.0505 - accuracy: 0.9873 - val_loss: 0.0490 - val_accuracy: 0.9866\n",
      "Q 675+234 T 909  ☑ 909 \n",
      "Q 823+656 T 1479 ☑ 1479\n",
      "Q 526+41  T 567  ☑ 567 \n",
      "Q 483+656 T 1139 ☒ 1149\n",
      "Q 33+578  T 611  ☑ 611 \n",
      "Q 979+14  T 993  ☑ 993 \n",
      "Q 27+634  T 661  ☑ 661 \n",
      "Q 4+190   T 194  ☑ 194 \n",
      "Q 561+447 T 1008 ☑ 1008\n",
      "Q 6+859   T 865  ☑ 865 \n",
      "\n",
      "Iteration 13\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.0399 - accuracy: 0.9896 - val_loss: 0.0279 - val_accuracy: 0.9938\n",
      "Q 6+479   T 485  ☑ 485 \n",
      "Q 374+20  T 394  ☑ 394 \n",
      "Q 45+867  T 912  ☑ 912 \n",
      "Q 21+675  T 696  ☑ 696 \n",
      "Q 60+711  T 771  ☑ 771 \n",
      "Q 5+192   T 197  ☑ 197 \n",
      "Q 692+56  T 748  ☑ 748 \n",
      "Q 3+138   T 141  ☑ 141 \n",
      "Q 379+75  T 454  ☑ 454 \n",
      "Q 924+626 T 1550 ☑ 1550\n",
      "\n",
      "Iteration 14\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0418 - accuracy: 0.9887 - val_loss: 0.0364 - val_accuracy: 0.9898\n",
      "Q 62+26   T 88   ☑ 88  \n",
      "Q 777+700 T 1477 ☑ 1477\n",
      "Q 79+340  T 419  ☑ 419 \n",
      "Q 4+410   T 414  ☑ 414 \n",
      "Q 89+284  T 373  ☑ 373 \n",
      "Q 455+14  T 469  ☑ 469 \n",
      "Q 81+229  T 310  ☑ 310 \n",
      "Q 365+947 T 1312 ☑ 1312\n",
      "Q 465+99  T 564  ☑ 564 \n",
      "Q 8+583   T 591  ☑ 591 \n",
      "\n",
      "Iteration 15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0377 - accuracy: 0.9897 - val_loss: 0.0457 - val_accuracy: 0.9865\n",
      "Q 763+731 T 1494 ☑ 1494\n",
      "Q 441+14  T 455  ☑ 455 \n",
      "Q 31+303  T 334  ☑ 334 \n",
      "Q 31+649  T 680  ☑ 680 \n",
      "Q 611+36  T 647  ☑ 647 \n",
      "Q 411+5   T 416  ☑ 416 \n",
      "Q 31+580  T 611  ☑ 611 \n",
      "Q 15+792  T 807  ☑ 807 \n",
      "Q 344+786 T 1130 ☑ 1130\n",
      "Q 56+319  T 375  ☑ 375 \n",
      "\n",
      "Iteration 16\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0331 - accuracy: 0.9910 - val_loss: 0.0203 - val_accuracy: 0.9956\n",
      "Q 593+103 T 696  ☑ 696 \n",
      "Q 4+479   T 483  ☑ 483 \n",
      "Q 477+90  T 567  ☑ 567 \n",
      "Q 7+295   T 302  ☑ 302 \n",
      "Q 66+793  T 859  ☑ 859 \n",
      "Q 591+424 T 1015 ☑ 1015\n",
      "Q 63+180  T 243  ☑ 243 \n",
      "Q 25+518  T 543  ☑ 543 \n",
      "Q 20+746  T 766  ☑ 766 \n",
      "Q 14+220  T 234  ☑ 234 \n",
      "\n",
      "Iteration 17\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.0148 - accuracy: 0.9968 - val_loss: 0.1108 - val_accuracy: 0.9632\n",
      "Q 989+216 T 1205 ☑ 1205\n",
      "Q 28+623  T 651  ☑ 651 \n",
      "Q 279+440 T 719  ☑ 719 \n",
      "Q 255+473 T 728  ☑ 728 \n",
      "Q 414+545 T 959  ☑ 959 \n",
      "Q 959+795 T 1754 ☑ 1754\n",
      "Q 85+70   T 155  ☑ 155 \n",
      "Q 71+143  T 214  ☑ 214 \n",
      "Q 23+27   T 50   ☑ 50  \n",
      "Q 788+804 T 1592 ☑ 1592\n",
      "\n",
      "Iteration 18\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0234 - accuracy: 0.9937 - val_loss: 0.0249 - val_accuracy: 0.9927\n",
      "Q 923+736 T 1659 ☑ 1659\n",
      "Q 541+689 T 1230 ☑ 1230\n",
      "Q 599+4   T 603  ☑ 603 \n",
      "Q 336+3   T 339  ☑ 339 \n",
      "Q 25+79   T 104  ☑ 104 \n",
      "Q 4+695   T 699  ☑ 699 \n",
      "Q 821+304 T 1125 ☑ 1125\n",
      "Q 19+848  T 867  ☑ 867 \n",
      "Q 777+90  T 867  ☑ 867 \n",
      "Q 67+505  T 572  ☑ 572 \n",
      "\n",
      "Iteration 19\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.0326 - accuracy: 0.9911 - val_loss: 0.0097 - val_accuracy: 0.9981\n",
      "Q 37+55   T 92   ☑ 92  \n",
      "Q 83+73   T 156  ☑ 156 \n",
      "Q 579+46  T 625  ☑ 625 \n",
      "Q 757+929 T 1686 ☑ 1686\n",
      "Q 687+46  T 733  ☑ 733 \n",
      "Q 198+219 T 417  ☑ 417 \n",
      "Q 25+341  T 366  ☑ 366 \n",
      "Q 376+410 T 786  ☑ 786 \n",
      "Q 66+66   T 132  ☑ 132 \n",
      "Q 572+73  T 645  ☑ 645 \n",
      "\n",
      "Iteration 20\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.0255 - accuracy: 0.9931 - val_loss: 0.0313 - val_accuracy: 0.9915\n",
      "Q 268+207 T 475  ☑ 475 \n",
      "Q 9+196   T 205  ☑ 205 \n",
      "Q 21+586  T 607  ☑ 607 \n",
      "Q 12+403  T 415  ☑ 415 \n",
      "Q 43+389  T 432  ☑ 432 \n",
      "Q 348+27  T 375  ☑ 375 \n",
      "Q 702+89  T 791  ☑ 791 \n",
      "Q 160+31  T 191  ☑ 191 \n",
      "Q 875+27  T 902  ☑ 902 \n",
      "Q 455+14  T 469  ☑ 469 \n",
      "\n",
      "Iteration 21\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0236 - val_accuracy: 0.9925\n",
      "Q 9+493   T 502  ☑ 502 \n",
      "Q 7+42    T 49   ☑ 49  \n",
      "Q 315+141 T 456  ☑ 456 \n",
      "Q 39+776  T 815  ☑ 815 \n",
      "Q 71+143  T 214  ☑ 214 \n",
      "Q 968+19  T 987  ☑ 987 \n",
      "Q 669+807 T 1476 ☑ 1476\n",
      "Q 830+137 T 967  ☑ 967 \n",
      "Q 111+83  T 194  ☑ 194 \n",
      "Q 917+66  T 983  ☑ 983 \n",
      "\n",
      "Iteration 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 0.0051 - val_accuracy: 0.9994\n",
      "Q 9+184   T 193  ☑ 193 \n",
      "Q 2+509   T 511  ☑ 511 \n",
      "Q 293+78  T 371  ☑ 371 \n",
      "Q 1+325   T 326  ☑ 326 \n",
      "Q 11+411  T 422  ☑ 422 \n",
      "Q 7+95    T 102  ☑ 102 \n",
      "Q 409+1   T 410  ☑ 410 \n",
      "Q 450+213 T 663  ☑ 663 \n",
      "Q 19+47   T 66   ☑ 66  \n",
      "Q 613+686 T 1299 ☑ 1299\n",
      "\n",
      "Iteration 23\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.0132 - val_accuracy: 0.9967\n",
      "Q 452+8   T 460  ☑ 460 \n",
      "Q 21+975  T 996  ☑ 996 \n",
      "Q 75+807  T 882  ☑ 882 \n",
      "Q 912+50  T 962  ☑ 962 \n",
      "Q 256+27  T 283  ☑ 283 \n",
      "Q 58+994  T 1052 ☑ 1052\n",
      "Q 640+797 T 1437 ☑ 1437\n",
      "Q 87+531  T 618  ☑ 618 \n",
      "Q 740+879 T 1619 ☑ 1619\n",
      "Q 18+63   T 81   ☑ 81  \n",
      "\n",
      "Iteration 24\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0248 - accuracy: 0.9932 - val_loss: 0.0085 - val_accuracy: 0.9983\n",
      "Q 7+287   T 294  ☑ 294 \n",
      "Q 317+683 T 1000 ☑ 1000\n",
      "Q 220+213 T 433  ☑ 433 \n",
      "Q 6+147   T 153  ☑ 153 \n",
      "Q 111+121 T 232  ☑ 232 \n",
      "Q 6+722   T 728  ☑ 728 \n",
      "Q 20+444  T 464  ☑ 464 \n",
      "Q 698+64  T 762  ☑ 762 \n",
      "Q 92+954  T 1046 ☑ 1046\n",
      "Q 90+943  T 1033 ☑ 1033\n",
      "\n",
      "Iteration 25\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "Q 91+489  T 580  ☑ 580 \n",
      "Q 599+1   T 600  ☑ 600 \n",
      "Q 323+32  T 355  ☑ 355 \n",
      "Q 583+627 T 1210 ☑ 1210\n",
      "Q 40+763  T 803  ☑ 803 \n",
      "Q 788+133 T 921  ☑ 921 \n",
      "Q 38+251  T 289  ☑ 289 \n",
      "Q 5+642   T 647  ☑ 647 \n",
      "Q 270+829 T 1099 ☑ 1099\n",
      "Q 6+13    T 19   ☑ 19  \n",
      "\n",
      "Iteration 26\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0268 - accuracy: 0.9924 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
      "Q 836+48  T 884  ☑ 884 \n",
      "Q 52+51   T 103  ☑ 103 \n",
      "Q 530+2   T 532  ☑ 532 \n",
      "Q 785+434 T 1219 ☑ 1219\n",
      "Q 67+971  T 1038 ☑ 1038\n",
      "Q 855+51  T 906  ☑ 906 \n",
      "Q 58+567  T 625  ☑ 625 \n",
      "Q 374+38  T 412  ☑ 412 \n",
      "Q 87+73   T 160  ☑ 160 \n",
      "Q 61+19   T 80   ☑ 80  \n",
      "\n",
      "Iteration 27\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.0066 - val_accuracy: 0.9987\n",
      "Q 8+316   T 324  ☑ 324 \n",
      "Q 821+304 T 1125 ☑ 1125\n",
      "Q 996+95  T 1091 ☑ 1091\n",
      "Q 675+234 T 909  ☑ 909 \n",
      "Q 6+253   T 259  ☑ 259 \n",
      "Q 742+187 T 929  ☑ 929 \n",
      "Q 59+481  T 540  ☑ 540 \n",
      "Q 76+517  T 593  ☑ 593 \n",
      "Q 273+1   T 274  ☑ 274 \n",
      "Q 737+77  T 814  ☑ 814 \n",
      "\n",
      "Iteration 28\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0203 - accuracy: 0.9944 - val_loss: 0.0161 - val_accuracy: 0.9955\n",
      "Q 96+494  T 590  ☑ 590 \n",
      "Q 376+951 T 1327 ☑ 1327\n",
      "Q 22+191  T 213  ☑ 213 \n",
      "Q 352+963 T 1315 ☑ 1315\n",
      "Q 6+420   T 426  ☒ 425 \n",
      "Q 53+167  T 220  ☑ 220 \n",
      "Q 511+810 T 1321 ☑ 1321\n",
      "Q 174+52  T 226  ☑ 226 \n",
      "Q 4+884   T 888  ☑ 888 \n",
      "Q 909+25  T 934  ☑ 934 \n",
      "\n",
      "Iteration 29\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0044 - val_accuracy: 0.9991\n",
      "Q 7+623   T 630  ☑ 630 \n",
      "Q 778+616 T 1394 ☑ 1394\n",
      "Q 684+980 T 1664 ☑ 1664\n",
      "Q 42+19   T 61   ☑ 61  \n",
      "Q 15+88   T 103  ☑ 103 \n",
      "Q 382+91  T 473  ☑ 473 \n",
      "Q 141+31  T 172  ☑ 172 \n",
      "Q 403+147 T 550  ☑ 550 \n",
      "Q 6+479   T 485  ☑ 485 \n",
      "Q 773+697 T 1470 ☑ 1470\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
